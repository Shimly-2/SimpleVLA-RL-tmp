import torch
import numpy as np
from collections import defaultdict

def setup_example_data():
    """è®¾ç½®ç¤ºä¾‹æ•°æ®ï¼š(bs=8, 64, 56)"""
    bs = 8
    traj_len = 64
    action_dim = 56
    
    # æ¨¡æ‹Ÿ8ä¸ªæœºå™¨äººè½¨è¿¹
    torch.manual_seed(42)
    
    # åŠ¨ä½œåºåˆ— (bs, traj_len, action_dim)
    action_sequences = torch.randn(bs, traj_len, action_dim) * 0.5
    
    # å¥–åŠ±åªåœ¨æœ€åä¸€ä¸ªtokenï¼Œ0/1å¥–åŠ±
    token_level_rewards = torch.zeros(bs, traj_len)
    final_rewards = torch.tensor([0, 1, 0, 1, 1, 0, 1, 0]).float()  # 4æˆåŠŸ4å¤±è´¥
    token_level_rewards[:, -1] = final_rewards
    
    # EOS mask (æ‰€æœ‰ä½ç½®éƒ½æœ‰æ•ˆ)
    eos_mask = torch.ones(bs, traj_len)
    
    # æ¨¡æ‹Ÿå¤šä¸ªpromptç»„
    index = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1])  # ä¸¤ä¸ªpromptç»„
    
    print("=" * 60)
    print("ğŸ¤– è¾“å…¥æ•°æ®è®¾ç½®")
    print("=" * 60)
    print(f"æ‰¹æ¬¡å¤§å°: {bs}")
    print(f"è½¨è¿¹é•¿åº¦: {traj_len}")
    print(f"åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"æœ€ç»ˆå¥–åŠ±: {final_rewards.tolist()}")
    print(f"Promptç´¢å¼•: {index.tolist()}")
    print(f"æˆåŠŸç‡: {final_rewards.mean():.1%}")
    print()
    
    return action_sequences, token_level_rewards, eos_mask, index, final_rewards

def verl_original_grpo(token_level_rewards: torch.Tensor,
                       eos_mask: torch.Tensor,
                       index: torch.Tensor,
                       epsilon: float = 1e-6):
    """VERLæ¡†æ¶ä¸­çš„åŸå§‹GRPOå®ç°"""
    print("ğŸ”µ VERLåŸå§‹GRPOè®¡ç®—")
    print("-" * 40)
    
    response_length = token_level_rewards.shape[-1]
    scores = token_level_rewards.sum(dim=-1)
    
    print(f"å„è½¨è¿¹æ€»åˆ†: {scores.tolist()}")
    print(f"Promptåˆ†ç»„: {index.tolist()}")
    
    # id2scoreçš„valueæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ¯ç»„çš„å¥–åŠ±å€¼
    id2score = defaultdict(list)
    id2mean = {}
    id2std = {}
    
    with torch.no_grad():
        bsz = scores.shape[0]
        
        # æŒ‰promptç´¢å¼•åˆ†ç»„
        for i in range(bsz):
            id2score[index[i].item()].append(scores[i])
        
        print("\nå„ç»„ç»Ÿè®¡:")
        for idx in id2score:
            group_scores = torch.stack(id2score[idx])
            print(f"ç»„ {idx}: åˆ†æ•°={[f'{x:.1f}' for x in id2score[idx]]}")
            
            if len(id2score[idx]) == 1:
                # åªæœ‰ä¸€ä¸ªæ ·æœ¬æ—¶
                id2mean[idx] = torch.tensor(0.0)
                id2std[idx] = torch.tensor(1.0)
                print(f"      å•æ ·æœ¬ -> å‡å€¼=0.0, æ ‡å‡†å·®=1.0")
            elif len(id2score[idx]) > 1:
                # å¤šä¸ªæ ·æœ¬æ—¶è®¡ç®—çœŸå®ç»Ÿè®¡é‡
                id2mean[idx] = torch.mean(group_scores)
                id2std[idx] = torch.std(group_scores, unbiased=False)  # VERLä½¿ç”¨æ€»ä½“æ ‡å‡†å·®
                print(f"      å‡å€¼={id2mean[idx]:.3f}, æ ‡å‡†å·®={id2std[idx]:.3f}")
        
        print("\næ ‡å‡†åŒ–è¿‡ç¨‹:")
        # æ ‡å‡†åŒ–
        normalized_scores = torch.zeros_like(scores)
        for i in range(bsz):
            original_score = scores[i]
            mean_val = id2mean[index[i].item()]
            std_val = id2std[index[i].item()]
            
            normalized_score = (original_score - mean_val) / (std_val + epsilon)
            normalized_scores[i] = normalized_score
            
            print(f"è½¨è¿¹{i}: ({original_score:.1f} - {mean_val:.3f}) / {std_val:.3f} = {normalized_score:.3f}")
        
        # æ‰©å±•åˆ°æ•´ä¸ªåºåˆ—
        advantages = normalized_scores.unsqueeze(-1).tile([1, response_length]) * eos_mask
    
    print(f"\nä¼˜åŠ¿ç»Ÿè®¡: å‡å€¼={advantages.mean():.3f}, æ ‡å‡†å·®={advantages.std():.3f}")
    print()
    return advantages, normalized_scores

def distributional_grpo_corrected(token_level_rewards: torch.Tensor,
                                eos_mask: torch.Tensor,
                                index: torch.Tensor,
                                action_sequences: torch.Tensor,
                                num_quantiles: int = 8,
                                epsilon: float = 1e-6):
    """ä¿®æ­£åçš„åˆ†å¸ƒå¼GRPO"""
    print("ğŸŸ¢ åˆ†å¸ƒå¼GRPOè®¡ç®—")
    print("-" * 40)
    
    response_length = token_level_rewards.shape[-1]
    scores = token_level_rewards.sum(dim=-1)
    
    # æ·»åŠ åŠ¨ä½œå¹³æ»‘åº¦å¥–åŠ±
    # action_diff = torch.diff(action_sequences, dim=1)
    # smoothness = 1.0 / (1.0 + torch.mean(torch.norm(action_diff, dim=-1), dim=-1))
    # enhanced_scores = scores + 0.1 * smoothness  # å¹³æ»‘åº¦æƒé‡0.1
    enhanced_scores = scores
    
    # print(f"åŸå§‹åˆ†æ•°: {scores.tolist()}")
    # print(f"å¹³æ»‘åº¦åˆ†æ•°: {[f'{x:.3f}' for x in smoothness.tolist()]}")
    # print(f"å¢å¼ºååˆ†æ•°: {[f'{x:.3f}' for x in enhanced_scores.tolist()]}")
    
    # æŒ‰ç»„æ„å»ºåˆ†ä½æ•°åˆ†å¸ƒ
    id2score_distribution = defaultdict(list)
    id2quantiles = {}
    
    with torch.no_grad():
        bsz = enhanced_scores.shape[0]
        for i in range(bsz):
            id2score_distribution[index[i].item()].append(enhanced_scores[i])
        
        print("\nå„ç»„åˆ†ä½æ•°:")
        for idx in id2score_distribution:
            scores_tensor = torch.stack(id2score_distribution[idx])
            
            if len(scores_tensor) >= num_quantiles:
                quantiles = torch.quantile(scores_tensor, 
                                         torch.linspace(0, 1, num_quantiles))
            else:
                # æ•°æ®ä¸è¶³æ—¶ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè¿‘ä¼¼
                mean = scores_tensor.mean()
                std = scores_tensor.std() + epsilon
                normal_points = torch.linspace(-2, 2, num_quantiles)
                quantiles = mean + std * normal_points
            
            id2quantiles[idx] = quantiles
            print(f"ç»„ {idx}: åˆ†ä½æ•°={[f'{x:.3f}' for x in quantiles.tolist()]}")
        
        print("\nåˆ†å¸ƒä¼˜åŠ¿è®¡ç®—:")
        distributional_advantages = torch.zeros_like(token_level_rewards)
        normalized_scores = torch.zeros(bsz)
        
        for i in range(bsz):
            current_score = enhanced_scores[i]
            quantiles = id2quantiles[index[i].item()]
            
            # è®¡ç®—ç™¾åˆ†ä½æ’å
            rank = torch.searchsorted(quantiles, current_score, right=True)
            percentile = rank.float() / len(quantiles)
            percentile = torch.clamp(percentile, 0.0, 1.0)
            
            # åˆ†å¸ƒä¼˜åŠ¿ï¼š[-1, 1]
            dist_advantage = 2 * (percentile - 0.5)
            normalized_scores[i] = dist_advantage
            
            # æ—¶é—´æƒé‡ (æŒ‡æ•°è¡°å‡ï¼Œåé¢æƒé‡æ›´é«˜)
            time_weights = torch.pow(0.95, torch.arange(response_length, 0, -1, dtype=torch.float))
            
            distributional_advantages[i] = dist_advantage * time_weights * eos_mask[i]
            
            print(f"è½¨è¿¹{i}: åˆ†æ•°={current_score:.3f}, ç™¾åˆ†ä½={percentile:.3f}, ä¼˜åŠ¿={dist_advantage:.3f}")
    
    print(f"\nä¼˜åŠ¿ç»Ÿè®¡: å‡å€¼={distributional_advantages.mean():.3f}, æ ‡å‡†å·®={distributional_advantages.std():.3f}")
    print()
    import ipdb;ipdb.set_trace()
    return distributional_advantages, normalized_scores

def momentum_grpo_corrected(token_level_rewards: torch.Tensor,
                          eos_mask: torch.Tensor,
                          index: torch.Tensor,
                          action_sequences: torch.Tensor,
                          momentum: float = 0.7,
                          epsilon: float = 1e-6):
    """ä¿®æ­£åçš„åŠ¨é‡GRPO"""
    print("ğŸŸ¡ åŠ¨é‡GRPOè®¡ç®—")
    print("-" * 40)
    
    # å…ˆè®¡ç®—å½“å‰æ‰¹æ¬¡çš„VERLæ ‡å‡†åŒ–ä¼˜åŠ¿
    _, current_normalized_scores = verl_original_grpo(token_level_rewards, eos_mask, index, epsilon)
    
    # æ¨¡æ‹Ÿå†å²ä¼˜åŠ¿ (åˆ†ç»„å­˜å‚¨)
    historical_advantages = {
        0: torch.tensor([0.2, -0.3, 0.1, -0.2]),    # ç»„0çš„å†å²
        1: torch.tensor([-0.1, 0.3, -0.1, 0.2])     # ç»„1çš„å†å²
    }
    
    print("å†å²ä¼˜åŠ¿:")
    for group_idx, hist_adv in historical_advantages.items():
        print(f"ç»„ {group_idx}: {[f'{x:.3f}' for x in hist_adv.tolist()]}")
    
    print(f"å½“å‰æ ‡å‡†åŒ–ä¼˜åŠ¿: {[f'{x:.3f}' for x in current_normalized_scores.tolist()]}")
    
    # åº”ç”¨åŠ¨é‡æœºåˆ¶
    momentum_advantages = torch.zeros_like(token_level_rewards)
    final_normalized_scores = torch.zeros_like(current_normalized_scores)
    
    with torch.no_grad():
        print("\nåŠ¨é‡æ›´æ–°:")
        group_counters = {0: 0, 1: 0}  # æ¯ç»„çš„è®¡æ•°å™¨
        
        for i in range(len(current_normalized_scores)):
            group_idx = index[i].item()
            local_idx = group_counters[group_idx]
            
            hist_adv = historical_advantages[group_idx][local_idx]
            curr_adv = current_normalized_scores[i]
            
            # è‡ªé€‚åº”åŠ¨é‡ (åŸºäºå˜åŒ–ç¨‹åº¦)
            change_magnitude = torch.abs(curr_adv - hist_adv)
            adaptive_momentum = momentum * torch.exp(-change_magnitude / 2.0)
            adaptive_momentum = torch.clamp(adaptive_momentum, 0.1, 0.99)
            
            # åŠ¨é‡æ›´æ–°
            final_adv = adaptive_momentum * hist_adv + (1 - adaptive_momentum) * curr_adv
            final_normalized_scores[i] = final_adv
            
            # æ‰©å±•åˆ°æ•´ä¸ªåºåˆ—
            momentum_advantages[i] = final_adv * eos_mask[i]
            
            print(f"è½¨è¿¹{i}: å†å²={hist_adv:.3f}, å½“å‰={curr_adv:.3f}, "
                  f"åŠ¨é‡={adaptive_momentum:.3f}, æœ€ç»ˆ={final_adv:.3f}")
            
            group_counters[group_idx] += 1
    
    print(f"\nä¼˜åŠ¿ç»Ÿè®¡: å‡å€¼={momentum_advantages.mean():.3f}, æ ‡å‡†å·®={momentum_advantages.std():.3f}")
    print()
    return momentum_advantages, final_normalized_scores

def compare_key_differences():
    """å¯¹æ¯”å…³é”®å·®å¼‚"""
    print("ğŸš€ GRPOç®—æ³•å…³é”®å·®å¼‚å¯¹æ¯”")
    print("=" * 60)
    
    # è®¾ç½®æ•°æ®
    action_sequences, token_level_rewards, eos_mask, index, final_rewards = setup_example_data()
    
    # è¿è¡Œå„ç§ç®—æ³•
    original_adv, original_scores = verl_original_grpo(token_level_rewards, eos_mask, index)
    dist_adv, dist_scores = distributional_grpo_corrected(token_level_rewards, eos_mask, index, action_sequences)
    momentum_adv, momentum_scores = momentum_grpo_corrected(token_level_rewards, eos_mask, index, action_sequences)
    
    # å…³é”®å¯¹æ¯”
    print("ğŸ“Š æ ‡å‡†åŒ–åˆ†æ•°å¯¹æ¯”")
    print("=" * 60)
    print(f"{'è½¨è¿¹':<4} {'å¥–åŠ±':<4} {'ç»„':<4} {'åŸå§‹GRPO':<12} {'åˆ†å¸ƒå¼GRPO':<12} {'åŠ¨é‡GRPO':<12}")
    print("-" * 60)
    
    for i in range(len(final_rewards)):
        print(f"{i:<4} {final_rewards[i]:<4.0f} {index[i]:<4} "
              f"{original_scores[i]:<12.3f} {dist_scores[i]:<12.3f} {momentum_scores[i]:<12.3f}")
    
    print()
    
    # æˆåŠŸvså¤±è´¥å¯¹æ¯”
    print("ğŸ¯ æˆåŠŸvså¤±è´¥è½¨è¿¹å¯¹æ¯”")
    print("-" * 40)
    
    success_mask = final_rewards == 1
    failure_mask = final_rewards == 0
    
    methods = {
        "åŸå§‹GRPO": original_scores,
        "åˆ†å¸ƒå¼GRPO": dist_scores,
        "åŠ¨é‡GRPO": momentum_scores
    }
    
    for name, scores in methods.items():
        success_mean = scores[success_mask].mean()
        failure_mean = scores[failure_mask].mean()
        gap = success_mean - failure_mean
        
        print(f"{name}:")
        print(f"  æˆåŠŸè½¨è¿¹ä¼˜åŠ¿: {success_mean:>7.3f}")
        print(f"  å¤±è´¥è½¨è¿¹ä¼˜åŠ¿: {failure_mean:>7.3f}")
        print(f"  ä¼˜åŠ¿å·®è·:     {gap:>7.3f}")
        print()
    
    # å…³é”®æ´å¯Ÿ
    print("ğŸ’¡ å…³é”®å‘ç°:")
    print("1. åŸå§‹GRPO: åŸºäºç»„å†…z-scoreæ ‡å‡†åŒ–ï¼Œ0/1å¥–åŠ±å¯¼è‡´æ ‡å‡†å·®ä¸º0.5")
    print("2. åˆ†å¸ƒå¼GRPO: è€ƒè™‘åŠ¨ä½œè´¨é‡ï¼Œèƒ½åŒºåˆ†ç›¸åŒå¥–åŠ±ä¸‹çš„ä¸åŒè¡¨ç°")
    print("3. åŠ¨é‡GRPO: å¹³æ»‘å†å²æ³¢åŠ¨ï¼Œè®­ç»ƒæ›´ç¨³å®š")
    print("4. ä½ çš„0/1ç¨€ç–å¥–åŠ±åœºæ™¯ç‰¹åˆ«é€‚åˆåˆ†å¸ƒå¼GRPOæ”¹è¿›")

if __name__ == "__main__":
    compare_key_differences()

# å•ç‹¬å±•ç¤ºVERLåŸå§‹GRPOçš„è¯¦ç»†è®¡ç®—
def detailed_verl_example():
    print("\n" + "="*60)
    print("ğŸ” VERLåŸå§‹GRPOè¯¦ç»†è®¡ç®—ç¤ºä¾‹")
    print("="*60)
    
    # ç®€åŒ–ç¤ºä¾‹ï¼šä¸¤ä¸ªç»„ï¼Œæ¯ç»„2ä¸ªè½¨è¿¹
    scores = torch.tensor([0.0, 1.0, 0.0, 1.0])  # ç»„0:[0,1], ç»„1:[0,1]
    index = torch.tensor([0, 0, 1, 1])
    
    print("è¾“å…¥:")
    print(f"åˆ†æ•°: {scores.tolist()}")
    print(f"åˆ†ç»„: {index.tolist()}")
    
    # åˆ†ç»„ç»Ÿè®¡
    id2score = defaultdict(list)
    for i, score in enumerate(scores):
        id2score[index[i].item()].append(score.item())
    
    print("\nåˆ†ç»„ç»Ÿè®¡:")
    for group_idx, group_scores in id2score.items():
        print(f"ç»„ {group_idx}: {group_scores}")
        
        if len(group_scores) > 1:
            mean = np.mean(group_scores)
            std = np.std(group_scores)  # æ€»ä½“æ ‡å‡†å·®
            print(f"         å‡å€¼={mean:.3f}, æ ‡å‡†å·®={std:.3f}")
            
            # æ ‡å‡†åŒ–
            normalized = [(s - mean) / std for s in group_scores]
            print(f"         æ ‡å‡†åŒ–={[f'{x:.3f}' for x in normalized]}")
    
    print("\nğŸ”‘ å…³é”®ç‚¹:")
    print("- å¯¹äº0/1å¥–åŠ±ï¼Œç»„å†…æ ‡å‡†å·® = 0.5")
    print("- æˆåŠŸè½¨è¿¹æ ‡å‡†åŒ–åˆ†æ•° = +1.0")  
    print("- å¤±è´¥è½¨è¿¹æ ‡å‡†åŒ–åˆ†æ•° = -1.0")
    print("- è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦æ”¹è¿›çš„åŸå› ï¼")

if __name__ == "__main__":
    compare_key_differences()
    detailed_verl_example()